{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e03918",
   "metadata": {},
   "source": [
    "# 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b99577e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\end-to-end-wine-quality-project\n",
      "Added c:\\end-to-end-wine-quality-project\\src to Python path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display current working directory\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Navigate to the project root directory if we're in the research folder\n",
    "if os.path.basename(os.getcwd()) == \"research\":\n",
    "    os.chdir(\"../\")\n",
    "    print(f\"Changed to project root: {os.getcwd()}\")\n",
    "\n",
    "# Add the src directory to the Python path for imports\n",
    "src_path = os.path.join(os.getcwd(), \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    print(f\"Added {src_path} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a093b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlProject package found in: ['C:\\\\end-to-end-wine-quality-project\\\\src\\\\mlProject']\n",
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Test importing the project modules\n",
    "try:\n",
    "    import mlProject\n",
    "    print(f\"mlProject package found in: {mlProject.__path__}\")\n",
    "    \n",
    "    # Import core modules\n",
    "    from mlProject.constants import *\n",
    "    from mlProject.utils.common import read_yaml, create_directories, get_size\n",
    "    from mlProject import logger\n",
    "    print(\"All imports successful!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Module import failed: {e}\")\n",
    "    print(\"Try installing the package in development mode: pip install -e .\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2d323f",
   "metadata": {},
   "source": [
    "# 2. Data Configuration Classes\n",
    "Define a dataclass to hold configuration for data ingestion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42a94962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import urllib.request as request\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    \"\"\"Configuration class for data ingestion parameters.\"\"\"\n",
    "    root_dir: Path\n",
    "    source_URL: str\n",
    "    local_data_file: Path\n",
    "    unzip_dir: Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec42fa",
   "metadata": {},
   "source": [
    "# 3. Implementing the Configuration Manager\n",
    "The Configuration Manager loads YAML configurations and creates component-specific configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "034ce4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    Handles loading configuration, parameters, and schema from YAML files\n",
    "    and provides component-specific configurations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "        \n",
    "        # Load configuration files\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        \n",
    "        # Create root artifacts directory\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Returns DataIngestionConfig object with parameters from config.yaml\n",
    "        \"\"\"\n",
    "        config = self.config.data_ingestion\n",
    "        \n",
    "        # Create data ingestion directory\n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        # Create and return the data ingestion configuration\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_URL=config.source_URL,\n",
    "            local_data_file=config.local_data_file,\n",
    "            unzip_dir=config.unzip_dir\n",
    "        )\n",
    "        \n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94862d6d",
   "metadata": {},
   "source": [
    "# 4. Data Ingestion Component\n",
    "Let's implement a robust data ingestion component with multiple download methods and error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2ed83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    \"\"\"\n",
    "    Handles downloading and extracting data files from various sources.\n",
    "    Supports direct URL downloads, Kaggle datasets, and GitHub repositories.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def _is_kaggle_url(self):\n",
    "        \"\"\"Check if the URL is from Kaggle.\"\"\"\n",
    "        return \"kaggle\" in self.config.source_URL.lower()\n",
    "    \n",
    "    def _is_github_url(self):\n",
    "        \"\"\"Check if the URL is from GitHub.\"\"\"\n",
    "        return \"github\" in self.config.source_URL.lower()\n",
    "    \n",
    "    def _download_using_kaggle_api(self):\n",
    "        \"\"\"Download dataset using Kaggle API if available.\"\"\"\n",
    "        try:\n",
    "            import kaggle\n",
    "            # Extract dataset name from URL or config\n",
    "            if \"/\" in self.config.source_URL:\n",
    "                dataset_name = self.config.source_URL.split(\"/\")[-1]\n",
    "            else:\n",
    "                dataset_name = self.config.source_URL\n",
    "                \n",
    "            # Create directory for the download\n",
    "            os.makedirs(os.path.dirname(self.config.local_data_file), exist_ok=True)\n",
    "            \n",
    "            # Download dataset\n",
    "            kaggle.api.dataset_download_files(\n",
    "                dataset_name,\n",
    "                path=os.path.dirname(self.config.local_data_file),\n",
    "                unzip=False\n",
    "            )\n",
    "            logger.info(f\"Downloaded dataset using Kaggle API: {dataset_name}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Kaggle API download failed: {e}\")\n",
    "            logger.info(\"Falling back to direct URL download\")\n",
    "            return False\n",
    "    \n",
    "    def download_file(self):\n",
    "        \"\"\"Download the data file from the source URL to the local path.\"\"\"\n",
    "        if os.path.exists(self.config.local_data_file):\n",
    "            file_size = get_size(Path(self.config.local_data_file))\n",
    "            logger.info(f\"File already exists of size: {file_size}\")\n",
    "            \n",
    "            # Verify it's a valid file (not an HTML error page)\n",
    "            if self._is_valid_file():\n",
    "                return\n",
    "            else:\n",
    "                logger.warning(\"Existing file appears to be invalid. Re-downloading...\")\n",
    "                os.remove(self.config.local_data_file)\n",
    "        \n",
    "        # Create directory for the download\n",
    "        os.makedirs(os.path.dirname(self.config.local_data_file), exist_ok=True)\n",
    "        \n",
    "        # Try Kaggle API first if it's a Kaggle URL\n",
    "        if self._is_kaggle_url() and self._download_using_kaggle_api():\n",
    "            return\n",
    "            \n",
    "        # Otherwise, use direct URL download\n",
    "        try:\n",
    "            logger.info(f\"Downloading file from {self.config.source_URL}\")\n",
    "            \n",
    "            # GitHub URLs may need to be adjusted for raw content\n",
    "            download_url = self.config.source_URL\n",
    "            if self._is_github_url() and \"/blob/\" in download_url:\n",
    "                download_url = download_url.replace(\"/blob/\", \"/raw/\")\n",
    "                \n",
    "            filename, headers = request.urlretrieve(\n",
    "                url=download_url,\n",
    "                filename=self.config.local_data_file\n",
    "            )\n",
    "            logger.info(f\"Downloaded {filename} with headers: \\n{headers}\")\n",
    "            \n",
    "            # Verify the downloaded file is valid\n",
    "            if not self._is_valid_file():\n",
    "                raise ValueError(\"Downloaded file is not valid. Check the URL and access permissions.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading file: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _is_valid_file(self):\n",
    "        \"\"\"\n",
    "        Check if the downloaded file is valid by examining its content.\n",
    "        Returns True if valid, False otherwise.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.config.local_data_file):\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            # Check if it's a ZIP file\n",
    "            with open(self.config.local_data_file, 'rb') as f:\n",
    "                magic_number = f.read(4)\n",
    "                # ZIP file signature is PK\\x03\\x04\n",
    "                is_zip = magic_number.startswith(b'PK\\x03\\x04')\n",
    "                \n",
    "                # If it's supposed to be a ZIP file, validate it\n",
    "                if self.config.local_data_file.suffix.lower() == '.zip' and not is_zip:\n",
    "                    logger.warning(f\"File has .zip extension but isn't a valid ZIP file\")\n",
    "                    return False\n",
    "                    \n",
    "                # Check if it's an HTML error page\n",
    "                f.seek(0)\n",
    "                content_start = f.read(1000).lower()\n",
    "                if b'<!doctype html>' in content_start or b'<html' in content_start:\n",
    "                    logger.warning(\"Downloaded file appears to be an HTML page, not data\")\n",
    "                    return False\n",
    "                    \n",
    "            return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error validating file: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def extract_zip_file(self):\n",
    "        \"\"\"Extract the downloaded ZIP file to the specified directory.\"\"\"\n",
    "        if not os.path.exists(self.config.local_data_file):\n",
    "            raise FileNotFoundError(f\"Zip file not found: {self.config.local_data_file}\")\n",
    "            \n",
    "        try:\n",
    "            # First verify it's a valid ZIP file\n",
    "            try:\n",
    "                with zipfile.ZipFile(self.config.local_data_file, 'r') as test_zip:\n",
    "                    file_list = test_zip.namelist()\n",
    "                    logger.info(f\"ZIP file contains {len(file_list)} files\")\n",
    "            except zipfile.BadZipFile:\n",
    "                # If the file extension is .csv, it might be a direct CSV file, not a ZIP\n",
    "                if self.config.local_data_file.suffix.lower() == '.csv':\n",
    "                    logger.info(\"The file is a CSV, not a ZIP file. Copying it to the unzip directory...\")\n",
    "                    \n",
    "                    # Create unzip directory\n",
    "                    os.makedirs(self.config.unzip_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Copy the CSV file to the unzip directory\n",
    "                    dest_file = os.path.join(self.config.unzip_dir, os.path.basename(self.config.local_data_file))\n",
    "                    shutil.copy2(self.config.local_data_file, dest_file)\n",
    "                    logger.info(f\"Copied CSV file to {dest_file}\")\n",
    "                    return\n",
    "                else:\n",
    "                    # If it's not a CSV, re-raise the error\n",
    "                    raise\n",
    "                    \n",
    "            # Create unzip directory\n",
    "            unzip_path = self.config.unzip_dir\n",
    "            os.makedirs(unzip_path, exist_ok=True)\n",
    "            \n",
    "            # Extract files\n",
    "            with zipfile.ZipFile(self.config.local_data_file, 'r') as zip_ref:\n",
    "                zip_ref.extractall(unzip_path)\n",
    "            \n",
    "            logger.info(f\"Extracted ZIP file to {unzip_path}\")\n",
    "            \n",
    "            # List the extracted files\n",
    "            extracted_files = os.listdir(unzip_path)\n",
    "            logger.info(f\"Extracted files: {extracted_files}\")\n",
    "            \n",
    "        except zipfile.BadZipFile as e:\n",
    "            logger.error(f\"Bad ZIP file: {e}\")\n",
    "            # If the file exists but is not a valid ZIP, examine its content\n",
    "            with open(self.config.local_data_file, 'rb') as f:\n",
    "                content_start = f.read(100)\n",
    "                logger.error(f\"File starts with: {content_start}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting ZIP file: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def initiate_data_ingestion(self):\n",
    "        \"\"\"\n",
    "        Execute the complete data ingestion process:\n",
    "        1. Download the data file\n",
    "        2. Extract it (if it's a ZIP file)\n",
    "        3. Return the path to the extracted data\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting data ingestion process\")\n",
    "        try:\n",
    "            self.download_file()\n",
    "            self.extract_zip_file()\n",
    "            logger.info(\"Data ingestion completed successfully\")\n",
    "            return self.config.unzip_dir\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data ingestion failed: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcb2772",
   "metadata": {},
   "source": [
    "# 5. Pipeline Execution\n",
    "Now let's execute the data ingestion pipeline with robust error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ab6c009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-12 13:01:18,531: INFO: 1890961549: >>> Data ingestion pipeline started <<<]\n",
      "[2025-05-12 13:01:18,542: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-05-12 13:01:18,557: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-05-12 13:01:18,574: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-05-12 13:01:18,580: INFO: common: created directory at: artifacts]\n",
      "[2025-05-12 13:01:18,585: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "[2025-05-12 13:01:18,586: INFO: 900315346: Starting data ingestion process]\n",
      "[2025-05-12 13:01:18,593: INFO: 900315346: File already exists of size: ~ 5 KB]\n",
      "[2025-05-12 13:01:18,597: ERROR: 900315346: Error validating file: 'str' object has no attribute 'suffix']\n",
      "[2025-05-12 13:01:18,601: WARNING: 900315346: Existing file appears to be invalid. Re-downloading...]\n",
      "[2025-05-12 13:01:18,612: WARNING: 900315346: Kaggle API download failed: No module named 'kaggle']\n",
      "[2025-05-12 13:01:18,616: INFO: 900315346: Falling back to direct URL download]\n",
      "[2025-05-12 13:01:18,619: INFO: 900315346: Downloading file from https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009/download]\n",
      "[2025-05-12 13:01:19,509: INFO: 900315346: Downloaded artifacts/data_ingestion/winequality-red.zip with headers: \n",
      "Content-Type: text/html; charset=utf-8\n",
      "Date: Mon, 12 May 2025 10:01:12 GMT\n",
      "Cache-Control: no-cache, no-store\n",
      "Pragma: no-cache\n",
      "Set-Cookie: ka_sessionid=48baf4d626899c86de6804a36c5d5c15; max-age=2626560; path=/\n",
      "Set-Cookie: CSRF-TOKEN=CfDJ8KvMat0eHzhGoPokVBGB7D3nXOv_n20F1bNok3ik4hXy_pa2dE4nEg4pBo-b_PFq24tx063NOz3MK9Hn53snbS8cVlgj4m4WS1SKEVvXLQ; path=/; secure; samesite=strict; httponly\n",
      "Set-Cookie: XSRF-TOKEN=CfDJ8KvMat0eHzhGoPokVBGB7D1ZojeijTAQwxWaF_amAA5hWFxthSXNQArFFm4NfN4OYAWcM7qhgl0j6nGCHqyXy-ysJjlhTSWUR2vMJ7eo2d-xnA; path=/; secure; samesite=strict\n",
      "Set-Cookie: CLIENT-TOKEN=eyJhbGciOiJub25lIiwidHlwIjoiSldUIn0.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.; path=/; secure; samesite=lax\n",
      "Set-Cookie: GCLB=CJSG4_3E1PHEXhAD; path=/; HttpOnly\n",
      "Vary: Accept-Encoding\n",
      "X-Frame-Options: SAMEORIGIN\n",
      "Strict-Transport-Security: max-age=63072000; includeSubDomains; preload\n",
      "Content-Security-Policy: object-src 'none'; script-src 'nonce-dAhlFPDgoGGyvHD5MupmoA==' 'report-sample' 'unsafe-inline' 'unsafe-eval' 'strict-dynamic' https: http:; base-uri 'none'; report-uri https://csp.withgoogle.com/csp/kaggle/20201130; frame-src 'self' https://www.kaggleusercontent.com https://www.youtube.com/embed/ https://polygraph-cool.github.io https://www.google.com/recaptcha/ https://www.docdroid.com https://www.docdroid.net https://kaggle-static.storage.googleapis.com https://kkb-production.jupyter-proxy.kaggle.net https://kkb-production.firebaseapp.com https://kaggle-metastore.firebaseapp.com https://apis.google.com https://content-sheets.googleapis.com/ https://accounts.google.com/ https://storage.googleapis.com https://docs.google.com https://drive.google.com https://calendar.google.com/ https://google.qualtrics.com/ ;\n",
      "X-Content-Type-Options: nosniff\n",
      "Referrer-Policy: strict-origin-when-cross-origin\n",
      "Via: 1.1 google\n",
      "Alt-Svc: h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000\n",
      "Connection: close\n",
      "Transfer-Encoding: chunked\n",
      "\n",
      "]\n",
      "[2025-05-12 13:01:19,513: ERROR: 900315346: Error validating file: 'str' object has no attribute 'suffix']\n",
      "[2025-05-12 13:01:19,517: ERROR: 900315346: Error downloading file: Downloaded file is not valid. Check the URL and access permissions.]\n",
      "[2025-05-12 13:01:19,522: ERROR: 900315346: Data ingestion failed: Downloaded file is not valid. Check the URL and access permissions.]\n",
      "[2025-05-12 13:01:19,527: ERROR: 1890961549: Data ingestion failed]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3060\\1890961549.py\", line 12, in execute_data_ingestion\n",
      "    data_path = data_ingestion.initiate_data_ingestion()\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3060\\900315346.py\", line 181, in initiate_data_ingestion\n",
      "    self.download_file()\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3060\\900315346.py\", line 80, in download_file\n",
      "    raise ValueError(\"Downloaded file is not valid. Check the URL and access permissions.\")\n",
      "ValueError: Downloaded file is not valid. Check the URL and access permissions.\n",
      "Data ingestion failed: Downloaded file is not valid. Check the URL and access permissions.\n",
      "\n",
      "Possible solutions:\n",
      "1. Check if the URL in config.yaml is correct and accessible\n",
      "2. For Kaggle datasets, ensure you have the Kaggle API installed and configured\n",
      "3. Verify your internet connection\n",
      "4. If the file isn't a ZIP, modify the data_ingestion component to handle the file type\n"
     ]
    }
   ],
   "source": [
    "def execute_data_ingestion():\n",
    "    \"\"\"Execute the complete data ingestion pipeline with error handling.\"\"\"\n",
    "    try:\n",
    "        logger.info(\">>> Data ingestion pipeline started <<<\")\n",
    "        \n",
    "        # Initialize configuration\n",
    "        config = ConfigurationManager()\n",
    "        data_ingestion_config = config.get_data_ingestion_config()\n",
    "        \n",
    "        # Execute data ingestion\n",
    "        data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "        data_path = data_ingestion.initiate_data_ingestion()\n",
    "        \n",
    "        logger.info(f\">>> Data ingestion completed. Data available at: {data_path} <<<\")\n",
    "        return data_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.exception(\"Data ingestion failed\")\n",
    "        raise e\n",
    "\n",
    "# Execute the pipeline\n",
    "try:\n",
    "    data_directory = execute_data_ingestion()\n",
    "    print(f\"Data ingestion successful! Data directory: {data_directory}\")\n",
    "except Exception as e:\n",
    "    print(f\"Data ingestion failed: {str(e)}\")\n",
    "    \n",
    "    # Suggest possible fixes\n",
    "    print(\"\\nPossible solutions:\")\n",
    "    print(\"1. Check if the URL in config.yaml is correct and accessible\")\n",
    "    print(\"2. For Kaggle datasets, ensure you have the Kaggle API installed and configured\")\n",
    "    print(\"3. Verify your internet connection\")\n",
    "    print(\"4. If the file isn't a ZIP, modify the data_ingestion component to handle the file type\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
